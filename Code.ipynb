{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc804096-330e-422f-80e2-5ed852c086b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install html5lib bs4 requests lingua-language-detector google-api-python-client google-auth-httplib2 google-auth-oauthlib google-analytics-data selenium --upgrade\n",
    "\n",
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "from datetime import date\n",
    "import calendar\n",
    "import os.path\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import concurrent.futures\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google.auth.transport.requests import Request                                             \n",
    "from google.oauth2.credentials import Credentials                                              \n",
    "from google_auth_oauthlib.flow import InstalledAppFlow                                       \n",
    "from googleapiclient.discovery import build                                                  \n",
    "from googleapiclient.errors import HttpError                                                   \n",
    "from google.oauth2 import service_account\n",
    "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "#Set-up English or Indonesian language detector for checking the webscraper\n",
    "detector = LanguageDetectorBuilder.from_languages(Language.ENGLISH,Language.INDONESIAN).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29975a6e-6d1e-436f-a836-08a44a321bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS AND OPTIONS\n",
    "\n",
    "#Sets Selenium Chrome driver to run in headless mode for efficiency\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "#Function to pull and parse html from a url using Selenium\n",
    "#Waits until the specified wait tag, h4 header for example, appears in the url\n",
    "#Overcomes javascripting present in certain websites\n",
    "def selscraper(url,waittype,waittag):\n",
    "    chrome_driver.get(url)\n",
    "    wait = WebDriverWait(chrome_driver,10)\n",
    "    wait.until(EC.presence_of_element_located((waittype,waittag)))\n",
    "    html_content = chrome_driver.page_source\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    return soup\n",
    "\n",
    "#Pulls the publication title using the basic Requests library and BeautifulSoup\n",
    "#Function will run again if an error is raised\n",
    "def fetch_title(url):\n",
    "    work = False\n",
    "    while work == False:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text)\n",
    "            title = soup.find(id='publication-title')\n",
    "            if title is None:\n",
    "                work = True\n",
    "                return None\n",
    "            else:\n",
    "                work = True\n",
    "                return title.get_text(\"\").replace('\\n','').strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            work = False\n",
    "\n",
    "#Returns the string most similar to the input string from a range of strings \n",
    "def get_most_similar_string(input_str,string_list):\n",
    "    similarity_scores = np.array([SequenceMatcher(None,input_str,s).ratio() for s in string_list])\n",
    "    most_similar_index = np.argmax(similarity_scores)\n",
    "    return string_list[most_similar_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd96edc3-298b-4584-9b1c-ca35a3c43799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEBSCRAPING PUBLICATION TITLES FROM CIPS AND NELITI\n",
    "\n",
    "#Initialise Chrome Driver for Selenium\n",
    "chrome_driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "#CIPS WEBSITE\n",
    "#Pulls all the English publication titles from the CIPS website using Selenium and saves them to english_titles\n",
    "#Retries if a title language error is detected, as it has happened in the past\n",
    "eng_titles_soup = selscraper(\"https://www.cips-indonesia.org/publications\",By.CSS_SELECTOR,'h4')\n",
    "english_titles = [x.get_text(\" \").strip() for x in eng_titles_soup.find_all('h4') if x.get_text(\" \") != 'CIPS has more than 90 publications']\n",
    "while str(detector.detect_language_of(str(english_titles[0]))).replace('Language.','').title() == 'Indonesian':\n",
    "    eng_titles_soup = selscraper(\"https://www.cips-indonesia.org/publications\",By.CSS_SELECTOR,'h4')\n",
    "    english_titles = [x.get_text(\" \").strip() for x in eng_titles_soup.find_all('h4') if x.get_text(\" \") != 'CIPS has more than 90 publications']\n",
    "\n",
    "#Repeats the same with the Indonesian titles\n",
    "ind_titles_soup = selscraper(\"https://www.cips-indonesia.org/publications?lang=id\",By.CSS_SELECTOR,'h4')\n",
    "indonesian_titles = [x.get_text(\" \").strip() for x in ind_titles_soup.find_all('h4') if x.get_text(\" \") != 'CIPS memiliki lebih dari 90 publikasi']\n",
    "while str(detector.detect_language_of(str(indonesian_titles[-1]))).replace('Language.','').title() == 'English':\n",
    "    ind_titles_soup = selscraper(\"https://www.cips-indonesia.org/publications?lang=id\",By.CSS_SELECTOR,'h4')\n",
    "    indonesian_titles = [x.get_text(\" \").strip() for x in ind_titles_soup.find_all('h4') if x.get_text(\" \") != 'CIPS memiliki lebih dari 90 publikasi']\n",
    "\n",
    "#Combines both language title lists into one\n",
    "all_titles = english_titles + indonesian_titles\n",
    "\n",
    "#Fixes the titles that do not match those on Google Analytics. Titles need to match for performance matching to upload to the dashboard\n",
    "for index,x in enumerate(all_titles):\n",
    "    if x == 'Perjanjian Regional Comprehensive Economic Partnership (RCEP):  Peluangnya bagi Indonesia dan Langkah Pemanfaatannya ",
    " Sebuah Perspektif Internal':\n",
    "        all_titles[index] = 'Perjanjian Regional Comprehensive Economic Partnership (RCEP): Peluangnya bagi Indonesia dan Langkah Pemanfaatannya\\u2028 Sebuah Perspektif Internal'\n",
    "    elif x == 'Menuju Sistem Agropangan yang Lebih Berkelanjutan di Indonesia':\n",
    "        all_titles[index] = 'Menuju Sistem Pertanian Pangan yang Lebih Berkelanjutan di Indonesia'\n",
    "\n",
    "#NELITI\n",
    "#Returns the number of pages filled with titles on the Neliti website\n",
    "neliti_soup = selscraper(\"https://repository.cips-indonesia.org/browse/all\",By.CLASS_NAME,'sr-title')\n",
    "no_pages = neliti_soup.find_all(class_='pages')[-1].find('a').get_text()\n",
    "\n",
    "#Loops through all the pages, pulls the titles, and adds them to nel_titles\n",
    "nel_titles = []\n",
    "for i in list(range(1,int(no_pages)+1,1)):\n",
    "    soup = selscraper(\"https://repository.cips-indonesia.org/browse/all?page=\"+str(i),By.CLASS_NAME,'sr-title')\n",
    "    page_titles = [x.get_text().replace('\\n','').strip() for x in soup.find_all(class_='sr-title')]\n",
    "    nel_titles.append(page_titles)\n",
    "nel_titles = [i for x in nel_titles for i in x]\n",
    "\n",
    "#Quits the Chrome Driver\n",
    "chrome_driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d3d7dda-9e69-4395-b47b-cb0c9436653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download progress 100.0\n"
     ]
    }
   ],
   "source": [
    "#PULLING CURRENT GOOGLE SHEETS DASHBOARD AND NELITI FILES\n",
    "\n",
    "#Sets the appropriate credentials to use the Google Sheets and Drive APIs\n",
    "#Opens a tab to sign in with Google using the Google Cloud credentials if no working token exists\n",
    "#In case of credential errors, delete existing token and credentials and redownload from Google Cloud\n",
    "def credentials():\n",
    "    scopes = [\"https://www.googleapis.com/auth/spreadsheets\",\"https://www.googleapis.com/auth/drive\"]\n",
    "    credentials = None\n",
    "    if os.path.exists(\"API_Files/token.json\"):\n",
    "        credentials = Credentials.from_authorized_user_file(\"API_Files/token.json\",scopes)\n",
    "    if not credentials or not credentials.valid:\n",
    "        if credentials and credentials.expired and credentials.refresh_token:\n",
    "            credentials.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\"API_Files/credentials.json\",scopes)\n",
    "            credentials = flow.run_local_server(port=0)\n",
    "        with open(\"API_Files/token.json\",\"w\") as token:\n",
    "            token.write(credentials.to_json())\n",
    "    return credentials\n",
    "credentials = credentials()\n",
    "\n",
    "#Builds the Google Sheets service and executes a batch get for the relevant sheets\n",
    "service = build(\"sheets\",\"v4\",credentials=credentials)\n",
    "ranges = [\"Website Downloads\",\"Neliti Downloads\",\"Neliti Views\"]\n",
    "batch_response = service.spreadsheets().values().batchGet(spreadsheetId=\"SPREADSHEETID\",ranges=ranges).execute()\n",
    "\n",
    "#Converts the the sheets into dataframes and sets the appropriate data types\n",
    "web_dl = batch_response.get(\"valueRanges\")[0].get(\"values\",[])\n",
    "web_dl = pd.DataFrame(web_dl[1:],columns=web_dl[0])\n",
    "web_dl = pd.concat([web_dl.iloc[:,:3],web_dl.iloc[:,3:].astype('int64')],axis=1)\n",
    "nel_dl = batch_response.get(\"valueRanges\")[1].get(\"values\",[])\n",
    "nel_dl = pd.DataFrame(nel_dl[1:],columns=nel_dl[0])\n",
    "nel_dl = pd.concat([nel_dl.iloc[:,:3],nel_dl.iloc[:,3:].astype('int64')],axis=1)\n",
    "nel_view = batch_response.get(\"valueRanges\")[2].get(\"values\",[])\n",
    "nel_view = pd.DataFrame(nel_view[1:],columns=nel_view[0])\n",
    "nel_view = pd.concat([nel_view.iloc[:,:3],nel_view.iloc[:,3:].astype('int64')],axis=1)\n",
    "\n",
    "#Checks the shapes of Neliti Downloads and Neliti Views, should be the same\n",
    "if nel_dl.shape != nel_view.shape:\n",
    "    print(\"Unify columns for Neliti views and Neliti downloads\")\n",
    "\n",
    "elif len(nel_dl.columns.tolist()) == len(nel_view.columns.tolist()):\n",
    "    \n",
    "    #Builds the Google Drive service and saves the file names and ids in the Neliti data folder to a dataframe\n",
    "    service = build(\"drive\",\"v3\",credentials=credentials)\n",
    "    neliti_files = pd.DataFrame(service.files().list(q=\"'FOLDERID' in parents\",fields=\"nextPageToken, files(id, name)\")\\\n",
    "                                .execute().get(\"files\",[]))\n",
    "\n",
    "    #Returns the ids and filenames for months in the Neliti data folder that are not currently in the dashboard\n",
    "    neliti_missing_ids = [neliti_files.loc[neliti_files['name'] == x,'id'].values[0]\\\n",
    "                          for x in neliti_files['name'].tolist() if x.replace('.xlsx','') not in nel_dl.columns.tolist()[3:]]\n",
    "    neliti_missing_filenames = [neliti_files.loc[neliti_files['name'] == x,'name'].values[0]\\\n",
    "                                for x in neliti_files['name'].tolist() if x.replace('.xlsx','') not in nel_dl.columns.tolist()[3:]]\n",
    "    \n",
    "    #Downloads the missing months' files (if any)\n",
    "    if len(neliti_missing_ids) != 0:\n",
    "        for id,name in zip(neliti_missing_ids,neliti_missing_filenames):\n",
    "            request = service.files().get_media(fileId=id)\n",
    "            byter = io.BytesIO()\n",
    "            downloader = MediaIoBaseDownload(fd=byter,request=request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                print('Download progress {0}'.format(status.progress() * 100))\n",
    "            byter.seek(0)\n",
    "            with open('Data/Neliti_Files/'+name,'wb') as f:\n",
    "                f.write(byter.read())\n",
    "                byter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcf00d29-671a-4e0e-9bfc-94a6699ca279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles on Website but not found on Google Analytics:\n",
      "Modernizing Indonesia’s Agriculture\n",
      "Memodernisasi Pertanian Indonesia\n",
      "Kerahasiaan Data dalam Peraturan Perundang-Undangan Perlindungan Data Pribadi di Indonesia\n"
     ]
    }
   ],
   "source": [
    "#MATCH GOOGLE ANALYTICS TITLES AND TITLES ON CIPS WEBSITE\n",
    "#Titles need to match to match the downloads\n",
    "#Modernizing Indonesia's Agriculture, Memodernisasi Pertanian Indonesia, and Kerahasiaan Data dalam Peraturan Perundang-Undangan Perlindungan Data Pribadi di Indonesia\n",
    "#The above do not have any downloads which is why they appear here. If other titles appear, check Google Analytics for the appropriate format\n",
    "\n",
    "#Uses service account credentials to setup the Google Analytics service\n",
    "credentials = service_account.Credentials.from_service_account_file('API_Files/SERVICEACCOUNT.JSON')\n",
    "client = BetaAnalyticsDataClient(credentials=credentials)\n",
    "\n",
    "#Runs a report pulling all titles and corresponding file downloads since January 2018 and saves the titles to a list\n",
    "report = client.run_report({\"property\":\"properties/ID\",\"date_ranges\":[{\"start_date\":\"2018-01-01\",\"end_date\":\"today\"}],\\\n",
    "                            \"dimensions\":[{\"name\":\"eventName\",\"name\":\"unifiedScreenClass\"}],\"metrics\":[{\"name\":\"eventCount\"}],\\\n",
    "                            \"dimension_filter\":{\"filter\":{\"field_name\":\"eventName\",\"string_filter\":{\"value\":\"file_download\"}}}})\n",
    "ganalytics_data = [row.dimension_values[0].value for row in report.rows]\n",
    "\n",
    "\n",
    "#Returns a list of titles from the webscraped titles which are not found on Google Analytics\n",
    "print(\"Titles on Website but not found on Google Analytics:\")\n",
    "missing_titles = [print(x) for x in all_titles if x not in ganalytics_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c82fec4e-1f22-4f50-9d95-1719ee8d8568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING NEW DATA TO WEBSITE DOWNLOADS\n",
    "\n",
    "#Loops through titles on the website but which have not been added to the dashboard\n",
    "for title in [x for x in all_titles if x not in web_dl['Title'].tolist()]:\n",
    "    print(f\"Title: '{title}'\")\n",
    "\n",
    "    #Asks the user to input the publication type from a list of 4 options\n",
    "    possible_types = ['Discussion Paper','Policy Brief','Book','Policy Paper']\n",
    "    for index,y in enumerate(possible_types):\n",
    "        print(f'{index+1}. {y}')\n",
    "    input_type = str(input('Please select a number from the list'))\n",
    "    while input_type not in list(map(str,range(1,len(possible_types)+1,1))):\n",
    "        input_type = str(input('Please select a number from the list'))\n",
    "    input_type = possible_types[int(input_type)-1]\n",
    "\n",
    "    #Asks the user to input the publication topic from a list of 6 options\n",
    "    possible_topics = ['Digital & Financial Literacy','Economic Opportunities','Community Livelihoods','Education','Food Security & Agriculture','Trade & Investment']\n",
    "    for index,z in enumerate(possible_topics):\n",
    "        print(f'{index+1}. {z}')\n",
    "    input_topic = str(input('Please select a number from the list'))\n",
    "    while input_topic not in list(map(str,range(1,len(possible_topics)+1,1))):\n",
    "        input_topic = str(input('Please select a number from the list'))\n",
    "    input_topic = possible_topics[int(input_topic)-1]\n",
    "\n",
    "    #Appends the new publication to the bottom of web_dl with the inputted type and topic\n",
    "    web_dl.loc[len(web_dl)] = [title,input_type,input_topic] + [0] * len([x for x in web_dl.columns.tolist() if x != 'Title' and x != 'Type' and x != 'Topic'])\n",
    "\n",
    "#Ensures the correct data types\n",
    "web_dl = pd.concat([web_dl.iloc[:,:3],web_dl.iloc[:,3:].astype('int64')],axis=1)\n",
    "\n",
    "#Loops through any months that should be in the dashboard but which are not added yet\n",
    "for month in [x[:7] for x in pd.date_range('2019-01-01',date.today(),freq='M').astype('str').tolist() if x[:7] not in web_dl.columns.tolist()[3:]]:\n",
    "    current_year, current_month = map(int,month.split('-'))\n",
    "    last_day = str(calendar.monthrange(current_year,current_month)[1])\n",
    "    report = client.run_report({\"property\":\"properties/ID\",\"date_ranges\":[{\"start_date\":month+\"-01\",\"end_date\":month+\"-\"+last_day}],\\\n",
    "                            \"dimensions\":[{\"name\":\"eventName\",\"name\":\"unifiedScreenClass\"}],\"metrics\":[{\"name\":\"eventCount\"}],\\\n",
    "                            \"dimension_filter\":{\"filter\":{\"field_name\":\"eventName\",\"string_filter\":{\"value\":\"file_download\"}}}})\n",
    "    ganalytics_data = pd.DataFrame([{'Page Title':row.dimension_values[0].value,'Downloads':row.metric_values[0].value} for row in report.rows])\n",
    "    ganalytics_data['Downloads'] = ganalytics_data['Downloads'].astype('int64')\n",
    "\n",
    "    #Adds the publication data for the missing months to the dataframe\n",
    "    web_dl[month] = [ganalytics_data.loc[ganalytics_data['Page Title'] == x,'Downloads'].values[0] if x in ganalytics_data['Page Title'].tolist() else 0 for x in web_dl['Title'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "add65707-5a77-413f-9850-1e115583c847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indonesian Food Trade Policy during COVID-19\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Add to dashboard? Y/N n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effects of High Food Prices on Non-Cash Food Subsidies (BPNT) in Indonesia - Case Study in East Nusa Tenggara\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Add to dashboard? Y/N n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Political Economy of Rice Policy in Indonesia: A Perspective on the ASEAN Economic Opportunity\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Add to dashboard? Y/N n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#ADDING NEW DATA TO NELITI DOWNLOADS & VIEWS\n",
    "\n",
    "#Loops through titles on Neliti which are not in the dashboard\n",
    "for title in [x for x in nel_titles if x not in nel_dl['Title'].tolist()]:\n",
    "    print(title)\n",
    "\n",
    "    #Not all titles should be added as there are some duplicates on the website. When unsure, check the website to see which one has data\n",
    "    cont = input('Add to dashboard? Y/N')\n",
    "    while cont.lower() != 'y' and cont.lower() != 'n':\n",
    "        cont = input('Press Y for yes or N for no')\n",
    "\n",
    "    #Skip the title if it should not be added\n",
    "    if cont == 'n':\n",
    "        continue\n",
    "\n",
    "    #Asks the user for publication type and topic like for Website Downloads then appends each to the Neliti Downloads and Neliti Views dataframes respectively\n",
    "    elif cont == 'y':\n",
    "        possible_types = ['Discussion Paper','Policy Brief','Book','Policy Paper']\n",
    "        for index,y in enumerate(possible_types):\n",
    "            print(f'{index+1}. {y}')\n",
    "        input_type = str(input('Please select a number from the list'))\n",
    "        while input_type not in list(map(str,range(1,len(possible_types)+1,1))):\n",
    "            input_type = str(input('Please select a number from the list'))\n",
    "        input_type = possible_types[int(input_type)-1]\n",
    "    \n",
    "        possible_topics = ['Digital & Financial Literacy','Economic Opportunities','Community Livelihoods','Education','Food Security & Agriculture','Trade & Investment']\n",
    "        for index,z in enumerate(possible_topics):\n",
    "            print(f'{index+1}. {z}')\n",
    "        input_topic = str(input('Please select a number from the list'))\n",
    "        while input_topic not in list(map(str,range(1,len(possible_topics)+1,1))):\n",
    "            input_topic = str(input('Please select a number from the list'))\n",
    "        input_topic = possible_topics[int(input_topic)-1]\n",
    "\n",
    "        nel_dl.loc[len(nel_dl)] = [title,input_type,input_topic] + [0] * len([x for x in nel_dl.columns.tolist() if x != 'Title' and x != 'Type' and x != 'Topic'])\n",
    "        nel_view.loc[len(nel_view)] = [title,input_type,input_topic] + [0] * len([x for x in nel_view.columns.tolist() if x != 'Title' and x != 'Type' and x != 'Topic'])\n",
    "\n",
    "#Loops through the downloaded month data. The files are only downloaded if missing from the dashboard\n",
    "for file in os.listdir(\"Data/Neliti_Files\"):\n",
    "\n",
    "    #Converts the Neliti data file into a dataframe and performs some adjustments including adding a label column for whether the row is a Download or View\n",
    "    month = pd.read_excel(\"Data/Neliti_Files/\"+file,sheet_name=0,names=['Pages','Hits'])\n",
    "    month['Pages'] = month['Pages'].str.replace('center-for-indonesian-policy-studies - ','https://')\n",
    "    month['Type'] = month['Pages'].apply(lambda x: 'Download' if '.pdf' in x else 'View')\n",
    "    month.sort_values('Type',ascending=False,inplace=True)\n",
    "\n",
    "    #Uses multithreading to return the publication title for all View data as each entry is a link to the relevant publication on neliti\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:\n",
    "        title_views = list(executor.map(fetch_title,month.loc[month['Type'] == 'View','Pages'].tolist()))\n",
    "\n",
    "    #For download data, extracts the relevant title section from the url\n",
    "    title_downloads = month.loc[month['Type'] == 'Download','Pages'].apply(lambda x: x.split('publications/')[1].split('-',1)[1].replace('.pdf','').replace('-',' ')).tolist()\n",
    "\n",
    "    #Adds the extracted titles to the dataframe\n",
    "    month['Month Titles'] = title_views + title_downloads\n",
    "\n",
    "    #Converts the extracted titles to the most similar title found in the Neliti dashboard\n",
    "    month['Dashboard Titles'] = [None if x is None else get_most_similar_string(x,nel_dl['Title'].tolist()) for x in month['Month Titles'].tolist()]\n",
    "\n",
    "    #Adds a new column for each missing month with the missing data to the Neliti Downloads and Neliti Views dataframes\n",
    "    nel_dl[file.replace('.xlsx','')] = [month.loc[(month['Dashboard Titles'] == x) & (month['Type'] == 'Download'),'Hits'].values[0]\\\n",
    "                                        if x in month.loc[month['Type'] == 'Download','Dashboard Titles'].tolist() else 0 for x in nel_dl['Title'].tolist()]\n",
    "    nel_view[file.replace('.xlsx','')] = [month.loc[(month['Dashboard Titles'] == x) & (month['Type'] == 'View'),'Hits'].values[0]\\\n",
    "                                          if x in month.loc[month['Type'] == 'View','Dashboard Titles'].tolist() else 0 for x in nel_view['Title'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2a8b82e-02c7-43b9-ab11-bf34be1634c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETING NELITI FILES AND UPLOADING DATAFRAMES TO DASHBOARD\n",
    "\n",
    "#Deletes all the downloaded files in the folder\n",
    "for file in os.listdir(\"Data/Neliti_Files\"):\n",
    "    os.remove(\"Data/Neliti_Files/\" + file)\n",
    "\n",
    "#Sets credentials\n",
    "def credentials():\n",
    "    scopes = [\"https://www.googleapis.com/auth/spreadsheets\",\"https://www.googleapis.com/auth/drive\"]\n",
    "    credentials = None\n",
    "    if os.path.exists(\"API_Files/token.json\"):\n",
    "        credentials = Credentials.from_authorized_user_file(\"API_Files/token.json\",scopes)\n",
    "    if not credentials or not credentials.valid:\n",
    "        if credentials and credentials.expired and credentials.refresh_token:\n",
    "            credentials.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\"API_Files/credentials.json\",scopes)\n",
    "            credentials = flow.run_local_server(port=0)\n",
    "        with open(\"API_Files/token.json\",\"w\") as token:\n",
    "            token.write(credentials.to_json())\n",
    "    return credentials\n",
    "credentials = credentials()\n",
    "\n",
    "#Sets the relevant ranges with the updated dataframes\n",
    "service = build(\"sheets\",\"v4\",credentials=credentials)\n",
    "body = {'values':[web_dl.columns.tolist()] + web_dl.values.tolist()}\n",
    "request = service.spreadsheets().values().update(spreadsheetId=\"SPREADSHEETID\",range='Website Downloads',valueInputOption=\"USER_ENTERED\",body=body).execute()\n",
    "body = {'values':[nel_dl.columns.tolist()] + nel_dl.values.tolist()}\n",
    "request = service.spreadsheets().values().update(spreadsheetId=\"SPREADSHEETID\",range='Neliti Downloads',valueInputOption=\"USER_ENTERED\",body=body).execute()\n",
    "body = {'values':[nel_view.columns.tolist()] + nel_view.values.tolist()}\n",
    "request = service.spreadsheets().values().update(spreadsheetId=\"SPREADSHEETID\",range='Neliti Views',valueInputOption=\"USER_ENTERED\",body=body).execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
