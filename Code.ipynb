{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc804096-330e-422f-80e2-5ed852c086b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install html5lib bs4 requests lingua-language-detector google-api-python-client google-auth-httplib2 google-auth-oauthlib google-analytics-data selenium --upgrade\n",
    "\n",
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "from datetime import date\n",
    "import calendar\n",
    "import os.path\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import concurrent.futures\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google.auth.transport.requests import Request                                             \n",
    "from google.oauth2.credentials import Credentials                                              \n",
    "from google_auth_oauthlib.flow import InstalledAppFlow                                       \n",
    "from googleapiclient.discovery import build                                                  \n",
    "from googleapiclient.errors import HttpError                                                   \n",
    "from google.oauth2 import service_account\n",
    "from google.analytics.data_v1beta import BetaAnalyticsDataClient\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "#Set-up English or Indonesian language detector for checking the webscraper\n",
    "detector = LanguageDetectorBuilder.from_languages(Language.ENGLISH,Language.INDONESIAN).build()\n",
    "\n",
    "#Pandas dataframe display settings\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29975a6e-6d1e-436f-a836-08a44a321bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS\n",
    "\n",
    "index_missing_titles_neliti = [21,63,95,116,118,119,137,164]\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "def selscraper(url,waittype,waittag):\n",
    "    chrome_driver.get(url)\n",
    "    wait = WebDriverWait(chrome_driver, 10)\n",
    "    wait.until(EC.presence_of_element_located((waittype,waittag)))\n",
    "    html_content = chrome_driver.page_source\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_most_similar_string(input_str, string_list):\n",
    "    similarity_scores = np.array([SequenceMatcher(None, input_str, s).ratio() for s in string_list])\n",
    "    most_similar_index = np.argmax(similarity_scores)\n",
    "    return string_list[most_similar_index]\n",
    "\n",
    "def fetch_title(url):\n",
    "    work = False\n",
    "    while work == False:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text)\n",
    "            title = soup.find(id='publication-title')\n",
    "            if title is None:\n",
    "                work = True\n",
    "                return None\n",
    "            else:\n",
    "                work = True\n",
    "                return title.get_text(\"\").replace('\\n','').strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            work = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd96edc3-298b-4584-9b1c-ca35a3c43799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEBSCRAPING PUBLICATION TITLES FROM CIPS AND NELITI\n",
    "#Selenium is needed because Wix uses javascript to dynamically load the page\n",
    "#The 'wait' function on Selenium allows the entire page to load first before pulling the html\n",
    "\n",
    "#CIPS WEBSITE\n",
    "#Initialise the chrome driver using the options above\n",
    "chrome_driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "eng_titles_soup = selscraper(\"https://www.cips-indonesia.org/publications\",By.CSS_SELECTOR,'h4')\n",
    "\n",
    "#Extracts the text from all located 'h4' in the html excluding the web page's title\n",
    "english_titles = [x.get_text(\" \").strip() for x in eng_titles_soup.find_all('h4') if x.get_text(\" \") != 'CIPS has more than 90 publications']\n",
    "\n",
    "while str(detector.detect_language_of(str(english_titles[0]))).replace('Language.','').title() == 'Indonesian':\n",
    "    eng_titles_soup = selscraper(\"https://www.cips-indonesia.org/publications\",By.CSS_SELECTOR,'h4')\n",
    "    english_titles = [x.get_text(\" \").strip() for x in eng_titles_soup.find_all('h4') if x.get_text(\" \") != 'CIPS has more than 90 publications']\n",
    "\n",
    "#Repeats the same with the Indonesian titles\n",
    "ind_titles_soup = selscraper(\"https://www.cips-indonesia.org/publications?lang=id\",By.CSS_SELECTOR,'h4')\n",
    "indonesian_titles = [x.get_text(\" \").strip() for x in ind_titles_soup.find_all('h4') if x.get_text(\" \") != 'CIPS memiliki lebih dari 90 publikasi']\n",
    "\n",
    "while str(detector.detect_language_of(str(indonesian_titles[-1]))).replace('Language.','').title() == 'English':\n",
    "    ind_titles_soup = selscraper(\"https://www.cips-indonesia.org/publications?lang=id\",By.CSS_SELECTOR,'h4')\n",
    "    indonesian_titles = [x.get_text(\" \").strip() for x in ind_titles_soup.find_all('h4') if x.get_text(\" \") != 'CIPS memiliki lebih dari 90 publikasi']\n",
    "\n",
    "#Combines both language title lists into one\n",
    "all_titles = english_titles + indonesian_titles\n",
    "\n",
    "for index,x in enumerate(all_titles):\n",
    "    if x == 'Perjanjian Regional Comprehensive Economic Partnership (RCEP):  Peluangnya bagi Indonesia dan Langkah Pemanfaatannya ",
    " Sebuah Perspektif Internal':\n",
    "        all_titles[index] = 'Perjanjian Regional Comprehensive Economic Partnership (RCEP): Peluangnya bagi Indonesia dan Langkah Pemanfaatannya\\u2028 Sebuah Perspektif Internal'\n",
    "    elif x == 'Menuju Sistem Agropangan yang Lebih Berkelanjutan di Indonesia':\n",
    "        all_titles[index] = 'Menuju Sistem Pertanian Pangan yang Lebih Berkelanjutan di Indonesia'\n",
    "\n",
    "#NELITI\n",
    "neliti_soup = selscraper(\"https://repository.cips-indonesia.org/browse/all\",By.CLASS_NAME,'sr-title')\n",
    "no_pages = neliti_soup.find_all(class_='pages')[-1].find('a').get_text()\n",
    "\n",
    "nel_titles = []\n",
    "for i in list(range(1,int(no_pages)+1,1)):\n",
    "    soup = selscraper(\"https://repository.cips-indonesia.org/browse/all?page=\"+str(i),By.CLASS_NAME,'sr-title')\n",
    "    page_titles = [x.get_text().replace('\\n','').strip() for x in soup.find_all(class_='sr-title')]\n",
    "    nel_titles.append(page_titles)\n",
    "nel_titles = [i for x in nel_titles for i in x]\n",
    "\n",
    "chrome_driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3d7dda-9e69-4395-b47b-cb0c9436653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PULLING CURRENT GOOGLE SHEETS DASHBOARD AND NELITI FILES\n",
    "\n",
    "#Define function for getting credentials\n",
    "def credentials():\n",
    "\n",
    "    #Define scope for relevant permissions and features\n",
    "    scopes = [\"https://www.googleapis.com/auth/spreadsheets\",\"https://www.googleapis.com/auth/drive\"]\n",
    "    \n",
    "    #Initialise blank credentials. Credentials necessary for using the API and accessing the appropriate sheets\n",
    "    credentials = None\n",
    "\n",
    "    #If there is an existing token.json, use this as the credentials. IF DOESN'T WORK THEN DELETE JSON AND CREDENTIALS AND REDOWNLOAD AUTHORISATION FROM GOOGLE CLOUD\n",
    "    if os.path.exists(\"API_Files/token.json\"):\n",
    "        credentials = Credentials.from_authorized_user_file(\"API_Files/token.json\",scopes)\n",
    "\n",
    "    #If no existing credentials or existing credentials are invalid\n",
    "    if not credentials or not credentials.valid:\n",
    "\n",
    "        #If existing credentials and they're expired and there is an available refresh token, request to refresh the token\n",
    "        if credentials and credentials.expired and credentials.refresh_token:\n",
    "            credentials.refresh(Request())\n",
    "\n",
    "        #If no existing credentials, runs new flow using the local json file and opens new window with request to login to Google account\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\"API_Files/credentials.json\",scopes)\n",
    "            credentials = flow.run_local_server(port=0)\n",
    "\n",
    "        #Writes a new json file with the credentials once logged in\n",
    "        with open(\"API_Files/token.json\",\"w\") as token:\n",
    "            token.write(credentials.to_json())\n",
    "    return credentials\n",
    "\n",
    "#Creates credentials\n",
    "credentials = credentials()\n",
    "\n",
    "#Setup the Google Sheets service using the generated credentials and specifies the ranges to get\n",
    "service = build(\"sheets\",\"v4\",credentials=credentials)\n",
    "ranges = [\"Website Downloads\",\"Neliti Downloads\",\"Neliti Views\"]\n",
    "\n",
    "#Make a batch request for the relevant sheets on the dashboard\n",
    "batch_response = service.spreadsheets().values().batchGet(spreadsheetId=\"1ACDkuDGNctxpdTk47watDaq2OystvRUOLheSotMwRts\",ranges=ranges).execute()\n",
    "\n",
    "#Assign sheet responses to respective variables\n",
    "web_dl = batch_response.get(\"valueRanges\")[0].get(\"values\",[])\n",
    "nel_dl = batch_response.get(\"valueRanges\")[1].get(\"values\",[])\n",
    "nel_view = batch_response.get(\"valueRanges\")[2].get(\"values\",[])\n",
    "\n",
    "#Converts the pulled data into dataframes with the appropriate data types\n",
    "web_dl = pd.DataFrame(web_dl[1:],columns=web_dl[0])\n",
    "web_dl = pd.concat([web_dl.iloc[:,:3],web_dl.iloc[:,3:].astype('int64')],axis=1)\n",
    "nel_dl = pd.DataFrame(nel_dl[1:],columns=nel_dl[0])\n",
    "nel_dl = pd.concat([nel_dl.iloc[:,:3],nel_dl.iloc[:,3:].astype('int64')],axis=1)\n",
    "nel_view = pd.DataFrame(nel_view[1:],columns=nel_view[0])\n",
    "nel_view = pd.concat([nel_view.iloc[:,:3],nel_view.iloc[:,3:].astype('int64')],axis=1)\n",
    "\n",
    "#Setup the Google Drive service\n",
    "service = build(\"drive\",\"v3\",credentials=credentials)\n",
    "\n",
    "#Returns a dataframe of all the files in the Neliti folder\n",
    "neliti_files = pd.DataFrame(service.files().list(q=\"'1lxX689eHFjmb6AJ-eRX831c2ypzYXkyO' in parents\",fields=\"nextPageToken, files(id, name)\").execute().get(\"files\",[]))\n",
    "\n",
    "if len(nel_dl.columns.tolist()) != len(nel_view.columns.tolist()):\n",
    "    print(\"Unify columns for Neliti views and Neliti downloads\")\n",
    "    \n",
    "elif len(nel_dl.columns.tolist()) == len(nel_view.columns.tolist()):\n",
    "    neliti_missing_months = [x for x in neliti_files['name'].tolist() if x.replace('.xlsx','') not in nel_dl.columns.tolist()[3:]]\n",
    "    neliti_missing_ids = [neliti_files.loc[neliti_files['name'] == x,'id'].values[0] for x in neliti_missing_months]\n",
    "    neliti_missing_filenames = [neliti_files.loc[neliti_files['name'] == x,'name'].values[0] for x in neliti_missing_months]\n",
    "    if len(neliti_missing_ids) != 0:\n",
    "        for id,name in zip(neliti_missing_ids,neliti_missing_filenames):\n",
    "            request = service.files().get_media(fileId=id)\n",
    "            byter = io.BytesIO()\n",
    "            downloader = MediaIoBaseDownload(fd=byter,request=request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                print('Download progress {0}'.format(status.progress() * 100))\n",
    "            byter.seek(0)\n",
    "            with open('Data/Neliti_Files/'+name,'wb') as f:\n",
    "                f.write(byter.read())\n",
    "                byter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf00d29-671a-4e0e-9bfc-94a6699ca279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles on Website but not found on Google Analytics:\n",
      "Modernizing Indonesia’s Agriculture\n",
      "Memodernisasi Pertanian Indonesia\n",
      "Kerahasiaan Data dalam Peraturan Perundang-Undangan Perlindungan Data Pribadi di Indonesia\n"
     ]
    }
   ],
   "source": [
    "#CHECKING WHETHER TITLES PULLED FROM WEBSITE ARE FOUND IN GOOGLE ANALYTICS FILE DOWNLOAD HISTORY\n",
    "#This is to check the formatting of the webscraped titles to exactly match the titles extracted from Google Analytics\n",
    "#'Modernizing Indonesia's Agriculture', 'Memodernisasi Pertanian Indonesia', and 'Kerahasiaan Data dalam Peraturan Perundang-Undangan Perlindungan Data Pribadi di Indonesia'\n",
    "#These three titles appear because they truly do not have any downloads since 2018\n",
    "#IF OTHER TITLES APPEAR, CHECK TITLE FORMAT\n",
    "\n",
    "#Activates credentials from the Google Cloud service account\n",
    "credentials = service_account.Credentials.from_service_account_file('API_Files/cips-publication-dashboard-da13a6d6f0fd.json')\n",
    "\n",
    "#Initialise the Google Analytics client using the credentials\n",
    "client = BetaAnalyticsDataClient(credentials=credentials)\n",
    "\n",
    "#Runs the report to pull the number of file downloads for each title in the time range 2018-01-01 to today\n",
    "report = client.run_report({\"property\":\"properties/353077506\",\"date_ranges\":[{\"start_date\":\"2018-01-01\",\"end_date\":\"today\"}],\\\n",
    "                            \"dimensions\":[{\"name\":\"eventName\",\"name\":\"unifiedScreenClass\"}],\"metrics\":[{\"name\":\"eventCount\"}],\\\n",
    "                            \"dimension_filter\":{\"filter\":{\"field_name\":\"eventName\",\"string_filter\":{\"value\":\"file_download\"}}}})\n",
    "\n",
    "#Initialise an empty list to append lists of the Google Analytics data. Each row contains one title and its downloads\n",
    "data = []\n",
    "\n",
    "#Loops through the report and appends lists to data with each row's data\n",
    "for row in report.rows:\n",
    "    dimension_values = [value.value for value in row.dimension_values]\n",
    "    metric_values = [value.value for value in row.metric_values]\n",
    "    data.append(dimension_values + metric_values)\n",
    "\n",
    "#Converts the list of list to a dataframe containing the Google Analytics data\n",
    "ganalytics_data = pd.DataFrame(data,columns=['Page Title','Downloads'])\n",
    "\n",
    "#Prints out all the titles in the webscraped all_titles which do not appear in the list of titles found in the Google Analytics data\n",
    "print(\"Titles on Website but not found on Google Analytics:\")\n",
    "missing_titles = [print(x) for x in all_titles if x not in ganalytics_data['Page Title'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c82fec4e-1f22-4f50-9d95-1719ee8d8568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING NEW ENTRIES AND MONTHS TO THE DASHBOARD'S WEBSITE DOWNLOADS AND COMPLETING THE NEW MONTHS WITH GOOGLE ANALYTICS DATA\n",
    "\n",
    "#Creates a list of all months from January 2019 to the month before the current month in the same format as found in web_dl\n",
    "start_to_today = [x[:7] for x in pd.date_range('2019-01-01',date.today(),freq='M').astype('str').tolist()]\n",
    "\n",
    "#Returns a list of all month columns in web_dl\n",
    "dates_in_web_dl = [x for x in web_dl.columns.tolist() if x != 'Title' and x != 'Type' and x != 'Topic']\n",
    "\n",
    "#Creates a list of missing months, if any\n",
    "missing_months = [x for x in start_to_today if x not in dates_in_web_dl]\n",
    "\n",
    "#Inserts new columns at the end of the click data for any missing months with a value of 0\n",
    "for x in missing_months:\n",
    "    web_dl.insert(len(web_dl.columns),x,[0] * len(web_dl))\n",
    "\n",
    "#Creates a list of titles found on the webscraped titles list but not in web_dl\n",
    "titles_not_in_web_dl = [x for x in all_titles if x not in web_dl['Title'].tolist()]\n",
    "\n",
    "#Loops through the missing titles\n",
    "for title in titles_not_in_web_dl:\n",
    "    print(f\"Title: '{title}'\")\n",
    "    \n",
    "    possible_types = ['Discussion Paper','Policy Brief','Book','Policy Paper']\n",
    "    for index,y in enumerate(possible_types):\n",
    "        print(f'{index+1}. {y}')\n",
    "    input_type = str(input('Please select a number from the list'))\n",
    "    while input_type not in list(map(str,range(1,len(possible_types)+1,1))):\n",
    "        input_type = str(input('Please select a number from the list'))\n",
    "    input_type = possible_types[int(input_type)-1]\n",
    "\n",
    "    possible_topics = ['Digital & Financial Literacy','Economic Opportunities','Community Livelihoods','Education','Food Security & Agriculture','Trade & Investment']\n",
    "    for index,z in enumerate(possible_topics):\n",
    "        print(f'{index+1}. {z}')\n",
    "    input_topic = str(input('Please select a number from the list'))\n",
    "    while input_topic not in list(map(str,range(1,len(possible_topics)+1,1))):\n",
    "        input_topic = str(input('Please select a number from the list'))\n",
    "    input_topic = possible_topics[int(input_topic)-1]\n",
    "\n",
    "    web_dl.loc[len(web_dl)] = [title,input_type,input_topic] + [0] * len([x for x in web_dl.columns.tolist() if x != 'Title' and x != 'Type' and x != 'Topic'])\n",
    "\n",
    "\n",
    "#Converts the data columns to integers\n",
    "web_dl = pd.concat([web_dl.iloc[:,:3],web_dl.iloc[:,3:].astype('int64')],axis=1)\n",
    "\n",
    "#Loops through the missing months \n",
    "for year in missing_months:\n",
    "\n",
    "    #Returns the integer values of the year and month from the string year\n",
    "    current_year, current_month = map(int,year.split('-'))\n",
    "\n",
    "    #Uses the calendar library to return the last day of the month\n",
    "    last_day = str(calendar.monthrange(current_year,current_month)[1])\n",
    "\n",
    "    #Runs Google Analytics file download report for the relevant month and appends to the dataframe, ganalytics_data, as before\n",
    "    report = client.run_report({\"property\":\"properties/353077506\",\"date_ranges\":[{\"start_date\":year+\"-01\",\"end_date\":year+\"-\"+last_day}],\\\n",
    "                            \"dimensions\":[{\"name\":\"eventName\",\"name\":\"unifiedScreenClass\"}],\"metrics\":[{\"name\":\"eventCount\"}],\\\n",
    "                            \"dimension_filter\":{\"filter\":{\"field_name\":\"eventName\",\"string_filter\":{\"value\":\"file_download\"}}}})\n",
    "    data = []\n",
    "    for row in report.rows:\n",
    "        dimension_values = [value.value for value in row.dimension_values]\n",
    "        metric_values = [value.value for value in row.metric_values]\n",
    "        data.append(dimension_values + metric_values)\n",
    "    ganalytics_data = pd.DataFrame(data,columns=['Page Title','Downloads'])\n",
    "    ganalytics_data['Downloads'] = ganalytics_data['Downloads'].astype('int64')\n",
    "\n",
    "    #Loops through the titles on web_dl\n",
    "    for title in web_dl['Title'].tolist():\n",
    "\n",
    "        #If the title is found in the Google Analytics report, adds the downloads to the web_dl dataframe for the particular month\n",
    "        if title in ganalytics_data['Page Title'].tolist():\n",
    "            web_dl.loc[web_dl['Title'] == title,year] = ganalytics_data.loc[ganalytics_data['Page Title'] == title,'Downloads'].values[0]\n",
    "        #If the title is not found, adds a 0\n",
    "        elif title not in ganalytics_data['Page Title'].tolist():\n",
    "            web_dl.loc[web_dl['Title'] == title,year] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "add65707-5a77-413f-9850-1e115583c847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indonesian Food Trade Policy during COVID-19\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Add to dashboard? Y/N n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effects of High Food Prices on Non-Cash Food Subsidies (BPNT) in Indonesia - Case Study in East Nusa Tenggara\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Add to dashboard? Y/N n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Political Economy of Rice Policy in Indonesia: A Perspective on the ASEAN Economic Opportunity\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Add to dashboard? Y/N n\n"
     ]
    }
   ],
   "source": [
    "titles_not_in_neliti_dash = [x for x in nel_titles if x not in nel_dl['Title'].tolist()]\n",
    "\n",
    "for title in titles_not_in_neliti_dash:\n",
    "    print(title)\n",
    "    cont = input('Add to dashboard? Y/N')\n",
    "    while cont.lower() != 'y' and cont.lower() != 'n':\n",
    "        cont = input('Press Y for yes or N for no')\n",
    "        \n",
    "    if cont == 'n':\n",
    "        continue\n",
    "\n",
    "    elif cont == 'y':\n",
    "        possible_types = ['Discussion Paper','Policy Brief','Book','Policy Paper']\n",
    "        for index,y in enumerate(possible_types):\n",
    "            print(f'{index+1}. {y}')\n",
    "        input_type = str(input('Please select a number from the list'))\n",
    "        while input_type not in list(map(str,range(1,len(possible_types)+1,1))):\n",
    "            input_type = str(input('Please select a number from the list'))\n",
    "        input_type = possible_types[int(input_type)-1]\n",
    "    \n",
    "        possible_topics = ['Digital & Financial Literacy','Economic Opportunities','Community Livelihoods','Education','Food Security & Agriculture','Trade & Investment']\n",
    "        for index,z in enumerate(possible_topics):\n",
    "            print(f'{index+1}. {z}')\n",
    "        input_topic = str(input('Please select a number from the list'))\n",
    "        while input_topic not in list(map(str,range(1,len(possible_topics)+1,1))):\n",
    "            input_topic = str(input('Please select a number from the list'))\n",
    "        input_topic = possible_topics[int(input_topic)-1]\n",
    "\n",
    "        nel_dl.loc[len(nel_dl)] = [title,input_type,input_topic] + [0] * len([x for x in nel_dl.columns.tolist() if x != 'Title' and x != 'Type' and x != 'Topic'])\n",
    "        nel_view.loc[len(nel_view)] = [title,input_type,input_topic] + [0] * len([x for x in nel_view.columns.tolist() if x != 'Title' and x != 'Type' and x != 'Topic'])\n",
    "\n",
    "for file in os.listdir(\"Data/Neliti_Files\"):\n",
    "    month = pd.read_excel(\"Data/Neliti_Files/\"+file,sheet_name=0,names=['Pages','Hits'])\n",
    "    month['Pages'] = month['Pages'].str.replace('center-for-indonesian-policy-studies - ','https://')\n",
    "    month['Type'] = month['Pages'].apply(lambda x: 'Download' if '.pdf' in x else 'View')\n",
    "    month = month.reindex(columns=['Pages','Type','Hits'])\n",
    "    \n",
    "    month_view = month.loc[month['Type'] == 'View'].copy()\n",
    "    urls = month_view['Pages'].tolist()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:\n",
    "        titles = list(executor.map(fetch_title,urls))\n",
    "    month_view['Month Titles'] = titles\n",
    "\n",
    "    month_dl = month.loc[month['Type'] == 'Download'].copy()\n",
    "    month_dl['Month Titles'] = month_dl['Pages'].apply(lambda x: x.split('publications/')[1].split('-',1)[1].replace('.pdf','').replace('-',' '))\n",
    "\n",
    "    month = pd.concat([month_view,month_dl],axis=0,ignore_index=True)    \n",
    "    \n",
    "    dash_titles = [None if x is None else get_most_similar_string(x,nel_dl['Title'].tolist()) for x in month['Month Titles'].tolist()]\n",
    "    month['Dashboard Titles'] = dash_titles\n",
    "\n",
    "    month_dl = month.loc[month['Type'] == 'Download']\n",
    "    nel_dl[file.replace('.xlsx','')] = [month_dl.loc[month_dl['Dashboard Titles'] == x,'Hits'].values[0] if x in month_dl['Dashboard Titles'].tolist() else 0 for x in nel_dl['Title'].tolist()]\n",
    "\n",
    "    month_view = month.loc[month['Type'] == 'View']\n",
    "    nel_view[file.replace('.xlsx','')] = [month_view.loc[month_view['Dashboard Titles'] == x,'Hits'].values[0] if x in month_view['Dashboard Titles'].tolist() else 0 for x in nel_view['Title'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2a8b82e-02c7-43b9-ab11-bf34be1634c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"Data/Neliti_Files\"):\n",
    "    os.remove(\"Data/Neliti_Files/\" + file)\n",
    "\n",
    "#Define function for getting credentials\n",
    "def credentials():\n",
    "\n",
    "    #Define scope for relevant permissions and features\n",
    "    scopes = [\"https://www.googleapis.com/auth/spreadsheets\",\"https://www.googleapis.com/auth/drive\"]\n",
    "    \n",
    "    #Initialise blank credentials. Credentials necessary for using the API and accessing the appropriate sheets\n",
    "    credentials = None\n",
    "\n",
    "    #If there is an existing token.json, use this as the credentials. IF DOESN'T WORK THEN DELETE JSON AND CREDENTIALS AND REDOWNLOAD AUTHORISATION FROM GOOGLE CLOUD\n",
    "    if os.path.exists(\"API_Files/token.json\"):\n",
    "        credentials = Credentials.from_authorized_user_file(\"API_Files/token.json\",scopes)\n",
    "\n",
    "    #If no existing credentials or existing credentials are invalid\n",
    "    if not credentials or not credentials.valid:\n",
    "\n",
    "        #If existing credentials and they're expired and there is an available refresh token, request to refresh the token\n",
    "        if credentials and credentials.expired and credentials.refresh_token:\n",
    "            credentials.refresh(Request())\n",
    "\n",
    "        #If no existing credentials, runs new flow using the local json file and opens new window with request to login to Google account\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\"API_Files/credentials.json\",scopes)\n",
    "            credentials = flow.run_local_server(port=0)\n",
    "\n",
    "        #Writes a new json file with the credentials once logged in\n",
    "        with open(\"API_Files/token.json\",\"w\") as token:\n",
    "            token.write(credentials.to_json())\n",
    "    return credentials\n",
    "\n",
    "#Creates credentials\n",
    "credentials = credentials()\n",
    "\n",
    "#Define try block for pulling current dashboard\n",
    "try:\n",
    "    #Setup the Google Sheets service using the generated credentials and specifies the ranges to get\n",
    "    service = build(\"sheets\",\"v4\",credentials=credentials)\n",
    "\n",
    "    body = {'values':[web_dl.columns.tolist()] + web_dl.values.tolist()}\n",
    "    request = service.spreadsheets().values().update(spreadsheetId=\"1ACDkuDGNctxpdTk47watDaq2OystvRUOLheSotMwRts\",range='Website Downloads',valueInputOption=\"USER_ENTERED\",body=body).execute()\n",
    "\n",
    "    body = {'values':[nel_dl.columns.tolist()] + nel_dl.values.tolist()}\n",
    "    request = service.spreadsheets().values().update(spreadsheetId=\"1ACDkuDGNctxpdTk47watDaq2OystvRUOLheSotMwRts\",range='Neliti Downloads',valueInputOption=\"USER_ENTERED\",body=body).execute()\n",
    "    \n",
    "    body = {'values':[nel_view.columns.tolist()] + nel_view.values.tolist()}\n",
    "    request = service.spreadsheets().values().update(spreadsheetId=\"1ACDkuDGNctxpdTk47watDaq2OystvRUOLheSotMwRts\",range='Neliti Views',valueInputOption=\"USER_ENTERED\",body=body).execute()\n",
    "    \n",
    "#Prints a http error if unable to pull\n",
    "except HttpError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526d9c0-aee1-476a-b552-a9ef1969553a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
